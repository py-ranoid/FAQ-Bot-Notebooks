{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Retreival Bot using IDFT\n",
    "### Steps\n",
    "- Loading and preprocessing Questions and Answers from dataset\n",
    "- Setting Stopwords\n",
    "- Intitialising and training TF_IDF vectors\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                   # To load and process dataset\n",
    "import numpy as np                    # For matrix operations\n",
    "from nltk.corpus import stopwords     # Using NLTK to load stopwords\n",
    "from nltk import wordpunct_tokenize   # Using NLTK to token sentences\n",
    "\n",
    "from beakerx import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option('display.width',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing Questions and Answers from dataset\n",
    "- `hdfc.pkl` : Collection of 1341 QnA about HDFC. (Scraped from HDFC's FAQ site)\n",
    "- Dropping stopwords\n",
    "- Stripping Questions of extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('hdfc.xlsx')\n",
    "df = df.drop_duplicates('Question')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac72f20ebbd491b8d35bf546f014418",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit = 1000\n",
    "reduced = df[['Question','Answer']][:limit]\n",
    "\n",
    "qlabels = reduced['Question'].to_dict()\n",
    "alabels = reduced['Answer'].to_dict()\n",
    "\n",
    "reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting stopwords\n",
    "- Import set of common stopwords from nltk\n",
    "- Adding domain-related stopword\n",
    "- Removing question words (To distinguish between intents of questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus = {'hdfc'}\n",
    "minus = {'what','how','where','when','why'}\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "stop.update(plus)\n",
    "stop.difference_update(minus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intitialising and training TF-IDF vectors\n",
    "- Setting stopwords to `stop`\n",
    "- `tf_vect` : `TfidfVectorizer` object. Can be used to convert strings to tf-idf vectors\n",
    "- `all_qs_vectors` : Matrix of TF-IDF vectors corresponding to questions in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc65f9c1c1f246f892c3ba9f3e0b2470",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_vect =TfidfVectorizer(stop_words=stop,\n",
    "                         lowercase=True,\n",
    "                         use_idf=True)\n",
    "all_qs_vectors = tf_vect.fit_transform(reduced['Question'])\n",
    "TableDisplay({\"Shape of all_qs_vectors :\":(all_qs_vectors.shape),\"Number of questions : \":all_qs_vectors.shape[0],\"Vocabulary size : \":all_qs_vectors.shape[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming context with tfidf\n",
    "context = 'How can I repay my Personal Loan?'\n",
    "context_vector = tf_vect.transform([context])\n",
    "context_matrix = context_vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253ae328ead941af843b8e4e5225c2a0",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying TF_IDF results\n",
    "def tabulate_vector(context):\n",
    "    values = []\n",
    "    for w in word_tokenize(context.strip()):\n",
    "        ind = tf_vect.vocabulary_.get(w.lower(),\"-\")\n",
    "        val = context_matrix[0,ind] if not ind == \"-\" else 0\n",
    "        values.append({\"Word\":w,\"Vocabulary Index\":str(ind),\"TF-IDF Value\":val})\n",
    "    TableDisplay(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Predicting closest question\n",
    "- `predict` has the following arguments\n",
    "    - `n`       : int  | Number of results (from top)\n",
    "    - `answers` : bool | Return answers or not\n",
    "    - `ret_best`: bool | Returns index of closest match\n",
    "- Steps for prediction\n",
    "    - Convert query to tfidf vector\n",
    "    - Get dot product of query vectors with each question to measures similarity\n",
    "    - Sort array indices by descending order of array values\n",
    "    - Return top n results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query,n=5,answers=False,ret_indices=False):\n",
    "    # Comparing context with all questions using dot product\n",
    "    query_vector = tf_vect.transform([query])\n",
    "    sim = np.dot(all_qs_vectors, query_vector.T)\n",
    "    # Converting numpy matrix to 1D array with 146 dot products (146 questions vs context)\n",
    "    arr = sim.toarray().flatten()\n",
    "    matches = arr.argsort(axis=0)[::-1]\n",
    "    top_n_matches = matches[:n]\n",
    "    results = []\n",
    "    if ret_indices:\n",
    "        return top_n_matches\n",
    "    for i in top_n_matches:\n",
    "        res = {\"Question\":qlabels[i],\"Ans\":alabels[i]} if answers else {\"Question\":qlabels[i],\"Score\":arr[i]}\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b47e03bace4be9bb6612f09710e918",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TableDisplay(predict('How do I pay my personal loan ?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding closest question by jaccard_distance\n",
    "- `tokens` is a dictionary mapping a question's index to a list of tokens in the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating tokens after converting to lowercase, removing stopwords and non-alphanumberic tokens\n",
    "# Note : nltk.word_tokenize does not split PIN/Pattern'\n",
    "tokens = {}\n",
    "for i in qlabels:\n",
    "    tokens[i] = set([x for x in wordpunct_tokenize(qlabels[i].lower()) if x.isalnum() and x not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating questions which have a jaccard_distance > 0.9 with another questions\n",
    "def get_jaccard_similarity(words,words2):\n",
    "    inter = words.intersection(words2)\n",
    "    union = words.union(words2)\n",
    "    return float(len(inter))/len(union),len(inter)\n",
    "\n",
    "def pred_jaccard(query,n=5):\n",
    "    words = set([x for x in wordpunct_tokenize(query.lower()) if x.isalnum() and x not in stop])\n",
    "    max_sim = -1\n",
    "    max_ind = None\n",
    "    scores = {}\n",
    "    for i in qlabels:        \n",
    "        sc = get_jaccard_similarity(words,tokens[i])\n",
    "        scores[i] = {\"question\":qlabels[i],\"score\":sc[0],\"inter\":sc[1]}\n",
    "    return pd.DataFrame(scores).T.sort_values('score',ascending=False)[:n]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8297b5a675ba41f3b547c1dcbdb1dd5b",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_jaccard('How does amortization work ?',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea577d7ae3914fe4a4ee0acf013138bd",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(predict('How does amortization work ?',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "useful         1\n",
       "alternate      1\n",
       "tat            1\n",
       "wants          1\n",
       "levied         1\n",
       "penalty        1\n",
       "attempts       1\n",
       "failure        1\n",
       "shall          1\n",
       "successful     1\n",
       "website        1\n",
       "store          1\n",
       "unblock        1\n",
       "blocked        1\n",
       "purchases      1\n",
       "fraudulent     1\n",
       "us             1\n",
       "equipment      1\n",
       "greeting       1\n",
       "sc             1\n",
       "mind           1\n",
       "things         1\n",
       "cardments      1\n",
       "stating        1\n",
       "exceeds        1\n",
       "authorized     1\n",
       "fresh          1\n",
       "wrongly        1\n",
       "lock           1\n",
       "interactive    1\n",
       "downloaded     1\n",
       "inactive       1\n",
       "platforms      1\n",
       "regenerate     1\n",
       "note           1\n",
       "mpin           1\n",
       "devices        1\n",
       "show           1\n",
       "storage        1\n",
       "months         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ = {}\n",
    "for x in list(tokens.values()):\n",
    "    for w in list(x):\n",
    "        all_[w] = all_.get(w,0)+1\n",
    "pd.Series(all_).sort_values()[:40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
